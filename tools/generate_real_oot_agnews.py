#!/usr/bin/env python3
"""
Script to generate LARGE-SCALE, DIVERSE OOT datasets.
Method: Downloads the raw AG News CSV directly from GitHub (Bypassing HF API limits).
Contains 120,000 unique samples to ensure diversity.
"""

import os
import yaml
import logging
import csv
import requests
import io

# ================= é…ç½®åŒºåŸŸ =================

DATA_SAVE_DIR = "/root/autodl-tmp/MemoryAgentBench/data/oot_generated"
CONFIG_SAVE_DIR = "/root/autodl-tmp/MemoryAgentBench/configs/data_conf/Test_Time_Learning/OOT_Generated"
PROJECT_ROOT = "/root/autodl-tmp/MemoryAgentBench"

# éœ€è¦ç”Ÿæˆçš„ OOT æ–‡ä»¶æ•°é‡
START_INDEX = 7
END_INDEX = 32
TOTAL_FILES_NEEDED = END_INDEX - START_INDEX + 1

# ç›®æ ‡æ–‡ä»¶å¤§å°ï¼šæ¯ä¸ª OOT æ–‡ä»¶å¤§çº¦ 1.5MB (çº¦ç­‰äºåŸæ–‡æœ¬é‡)
# 1.5 MB characters approx matches the token count needed.
TARGET_SIZE_PER_FILE = 1_500_000 

# AG News åŸå§‹ CSV ä¸‹è½½é“¾æ¥ (GitHub Raw, ä¸èµ° HF API)
CSV_URL = "https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv"

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def download_and_parse_csv():
    """ä¸‹è½½å¹¶è§£æåŒ…å« 120,000 æ¡æ•°æ®çš„ CSV"""
    logging.info("ğŸš€ Downloading raw CSV data (120k samples)...")
    try:
        response = requests.get(CSV_URL, timeout=60)
        response.raise_for_status()
        
        # è§£æ CSV
        # æ ¼å¼: Class Index, Title, Description
        content = response.content.decode('utf-8')
        csv_reader = csv.reader(io.StringIO(content), delimiter=',')
        
        samples = []
        label_map = {
            "1": "World",
            "2": "Sports",
            "3": "Business",
            "4": "Sci/Tech"
        }
        
        for row in csv_reader:
            if len(row) >= 3:
                label_idx = row[0]
                title = row[1]
                desc = row[2]
                
                label_text = label_map.get(label_idx, "News")
                full_text = f"{title}. {desc}"
                
                samples.append({
                    "text": full_text,
                    "label": label_text
                })
                
        logging.info(f"âœ… Successfully loaded {len(samples)} unique real samples.")
        return samples
        
    except Exception as e:
        logging.error(f"Failed to download raw CSV: {e}")
        return []

def format_sample(sample):
    """æ ¼å¼åŒ–ä¸º MemoryAgentBench éœ€è¦çš„ QA æ ¼å¼"""
    clean_text = sample['text'].replace("\n", " ").strip()
    return f"Question: {clean_text}\nlabel: {sample['label']}\n\n"

def main():
    # 1. å‡†å¤‡ç›®å½•
    os.makedirs(DATA_SAVE_DIR, exist_ok=True)
    os.makedirs(CONFIG_SAVE_DIR, exist_ok=True)
    
    # 2. è·å–æµ·é‡çœŸå®æ•°æ®
    all_samples = download_and_parse_csv()
    if not all_samples:
        return
    
    total_samples_count = len(all_samples)
    global_sample_ptr = 0 # å…¨å±€æŒ‡é’ˆï¼Œä¿è¯è·¨æ–‡ä»¶ä¹Ÿä¸é‡å¤ï¼ˆç›´åˆ°ç”¨å®Œä¸€è½®ï¼‰

    # 3. å¾ªç¯ç”Ÿæˆæ–‡ä»¶
    for i in range(TOTAL_FILES_NEEDED):
        current_file_id = START_INDEX + i
        file_content = ""
        current_file_size = 0
        
        # æŒç»­å¡«å……å†…å®¹ï¼Œç›´åˆ°è¾¾åˆ°ç›®æ ‡å¤§å° (1.5MB)
        while current_file_size < TARGET_SIZE_PER_FILE:
            # çº¿æ€§æå–ï¼Œä¿è¯ä¸é‡å¤
            sample = all_samples[global_sample_ptr % total_samples_count]
            global_sample_ptr += 1
            
            entry = format_sample(sample)
            file_content += entry
            current_file_size += len(entry)
        
        # 4. å†™å…¥ TXT æ•°æ®æ–‡ä»¶
        file_name = f"oot_gen_{current_file_id}.txt"
        file_path = os.path.join(DATA_SAVE_DIR, file_name)
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(file_content)
            
        # 5. å†™å…¥ YAML é…ç½®æ–‡ä»¶ (ä¸€æ¬¡æ€§è¡¥å…¨æ‰€æœ‰å­—æ®µ)
        config_name = f"OOT_Gen_{current_file_id}.yaml"
        config_path = os.path.join(CONFIG_SAVE_DIR, config_name)
        rel_path = os.path.relpath(file_path, PROJECT_ROOT)
        
        yaml_data = {
            "dataset": "Test_Time_Learning",
            "sub_dataset": "ag_news_real_oot",
            "file_path": rel_path,
            "name": f"oot_generated_{current_file_id}",
            "format": "text",
            "seed": 42 + i, # ç»™æ¯ä¸ªæ–‡ä»¶ä¸åŒçš„ seed
            "max_test_samples": 0,
            "split": "test",
            "description": f"Real AG News OOT dataset #{current_file_id} (No Repetition)"
        }
        
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(yaml_data, f, default_flow_style=False, sort_keys=False)
            
        logging.info(f"âœ… Generated OOT #{current_file_id}: Size {current_file_size/1024:.2f} KB | Samples from index {global_sample_ptr - (current_file_size // 200)} to {global_sample_ptr}")

if __name__ == "__main__":
    main()